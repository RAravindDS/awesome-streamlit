{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## learning langchain expression language \n",
    "from langchain_core.globals import set_debug \n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    google_api_key=GEMINI_API_KEY,\n",
    "    model=\"gemini-1.5-pro-latest\"\n",
    "    )\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_debug(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a expert on dealing depression problems, you have nearly 50+ years of experience. So you should give a perfect answer to the question. Please think, re-iterate and take a break and answer properly\"), \n",
    "    (\"human\", \"here is the question\\n {question}, and this is the hint from user: {hint}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = chat_template |  llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipe.invoke({\"hint\": \"because I woke up late\", \"question\": \"I'm so sad today, I don't know why this is because i was talking to two girls both are left now, they don't even think about me now\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's perfectly natural to feel sad and disappointed when connections with people you care about fade or change. It sounds like you're going through a tough time, and it's brave of you to reach out and share your feelings. \n",
      "\n",
      "Let's unpack this a bit. You mentioned feeling sad, and that two girls you were talking to aren't in your life the same way anymore.  That can be really hard, especially if you were hoping for something more to develop.  \n",
      "\n",
      "**Here's the thing about relationships:** They're complex. Sometimes people drift apart naturally, interests change, or life takes people in different directions. It doesn't always mean someone did something wrong, it's just the natural ebb and flow of life. \n",
      "\n",
      "**About that hint you gave:** Waking up late might seem like a small thing, but sometimes our mood can be affected by seemingly unrelated factors.  Maybe you missed out on something you were looking forward to, or it messed up your routine.  Our minds and bodies are connected, so don't discount the impact of physical things on your emotional state.\n",
      "\n",
      "**Here's what I want you to consider:**\n",
      "\n",
      "* **Don't jump to conclusions:** It's easy to assume the worst when people pull away.  Instead of blaming yourself or them, try reaching out with a simple, \"Hey, how are you doing?\" You might be surprised to find they've been busy or going through something themselves.\n",
      "* **Focus on what you can control:** You can't control how others feel or act, but you *can* control how you respond. Instead of dwelling on the negative, try focusing on things that bring you joy: hobbies, spending time with friends, or exploring new interests.\n",
      "* **Be kind to yourself:**  Rejection, even in subtle forms, hurts. Acknowledge your feelings, allow yourself to feel sad, but don't let it consume you.  Treat yourself with the same compassion and understanding you would offer a friend going through a similar situation.\n",
      "\n",
      "**Remember, you are not alone.**  Romantic relationships can be challenging, and it's normal to experience ups and downs. If you find yourself struggling to cope, consider talking to a trusted friend, family member, or mental health professional. They can offer support and guidance as you navigate these difficult emotions. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/myawesomegemini/.venv/lib/python3.11/site-packages/pydantic/_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "\n",
    "vecstore_a = DocArrayInMemorySearch.from_texts(\n",
    "    [\"half the info will be here\", \"James' birthday is the 7th December\"],\n",
    "    embedding=embeddings\n",
    ")\n",
    "vecstore_b = DocArrayInMemorySearch.from_texts(\n",
    "    [\"and half here\", \"James was born in 1994\"],\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_a = vecstore_a.as_retriever()\n",
    "retriever_b = vecstore_b.as_retriever()\n",
    "\n",
    "\n",
    "prompt_str = \"\"\"\n",
    "Answer the question properly and give the response. \n",
    "\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "\n",
    "template = ChatPromptTemplate.from_template(prompt_str) # you can also use the from_message function both are same. \n",
    "\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough \n",
    "\n",
    "\n",
    "source_chain = RunnableParallel(\n",
    "    {\"context\": retriever_a, \"question\":  RunnablePassthrough()} \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = source_chain | template | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The provided context does not contain any information about the person's death, so I cannot answer the question.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"When did you die?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resoruces \n",
    "# LCEL: https://www.pinecone.io/learn/series/langchain/langchain-expression-language/\n",
    "# Gemini llm: https://python.langchain.com/v0.1/docs/integrations/chat/google_generative_ai/\n",
    "# Embedding Gemini llm: https://github.com/google/generative-ai-docs/blob/main/examples/gemini/python/langchain/Gemini_LangChain_QA_Chroma_WebLoad.ipynb\n",
    "# Get your api key: https://aistudio.google.com/app/apikey\n",
    "# Get the gemini model names here: https://ai.google.dev/gemini-api/docs/models/gemini#model-variations\n",
    "# google generative ai docs: https://github.com/google/generative-ai-docs/tree/main\n",
    "# video_tutorial: https://www.youtube.com/watch?v=zREUGA_v3xc&ab_channel=MichaelDaigler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# understanding runnable parallel and runnable passthrough\n",
    "\n",
    "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough \n",
    "\n",
    "runnable = RunnableParallel(\n",
    "    passed=RunnablePassthrough(), \n",
    "    extra=RunnablePassthrough.assign( mult= lambda x: x[\"num\"]* 3), \n",
    "    modified=lambda x: x[\"num\"] + 1 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableParallel<passed,extra,modified>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"num\": 1\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableParallel<passed,extra,modified> > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"num\": 1\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableParallel<passed,extra,modified> > chain:RunnablePassthrough] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"num\": 1\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableParallel<passed,extra,modified> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"num\": 1\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableParallel<passed,extra,modified> > chain:RunnableLambda] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": 2\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableParallel<passed,extra,modified> > chain:RunnableAssign<mult>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"num\": 1\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableParallel<passed,extra,modified> > chain:RunnableAssign<mult> > chain:RunnableParallel<mult>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"num\": 1\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableParallel<passed,extra,modified> > chain:RunnableAssign<mult> > chain:RunnableParallel<mult> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"num\": 1\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableParallel<passed,extra,modified> > chain:RunnableAssign<mult> > chain:RunnableParallel<mult> > chain:RunnableLambda] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": 3\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableParallel<passed,extra,modified> > chain:RunnableAssign<mult> > chain:RunnableParallel<mult>] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"mult\": 3\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableParallel<passed,extra,modified> > chain:RunnableAssign<mult>] [5ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"num\": 1,\n",
      "  \"mult\": 3\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableParallel<passed,extra,modified>] [13ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"passed\": {\n",
      "    \"num\": 1\n",
      "  },\n",
      "  \"extra\": {\n",
      "    \"num\": 1,\n",
      "    \"mult\": 3\n",
      "  },\n",
      "  \"modified\": 2\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'passed': {'num': 1}, 'extra': {'num': 1, 'mult': 3}, 'modified': 2}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable.invoke({\"num\": 1})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
